{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Important URLs**\n",
        "\n",
        "\n",
        "**Autoencoders**\n",
        "\n",
        "https://keras.io/examples/vision/autoencoder/\n",
        "\n",
        "**VAE**\n",
        "\n",
        "https://keras.io/examples/generative/vae/\n",
        "\n",
        "**GANs**\n",
        "\n",
        "https://huggingface.co/learn/computer-vision-course/en/unit5/generative-models/gans\n",
        "\n",
        "https://keras.io/examples/generative/conditional_gan/\n",
        "\n",
        "**Transformers**\n",
        "\n",
        "https://www.geeksforgeeks.org/nlp/types-of-attention-mechanism/\n",
        "\n",
        "\n",
        "https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/\n",
        "\n",
        "\n",
        "**Huggingface Capabilities on Text**\n",
        "\n",
        "https://huggingface.co/learn/llm-course/en/chapter1/3\n",
        "\n",
        "\n",
        "**BERT**\n",
        "\n",
        "https://www.geeksforgeeks.org/nlp/explanation-of-bert-model-nlp/\n",
        "\n",
        "\n",
        "https://huggingface.co/docs/transformers/en/model_doc/bert\n"
      ],
      "metadata": {
        "id": "Oq0Jam-0k9Ym"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxCr71LxwFXV"
      },
      "source": [
        "### Convolutional autoencoder for image denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19E-PIwRwFXZ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement a deep convolutional autoencoder\n",
        "for image denoising, mapping noisy digits images from the MNIST dataset to\n",
        "clean digits images. This implementation is based on an original blog post\n",
        "titled [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
        "by [Fran√ßois Chollet](https://twitter.com/fchollet)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXtfRyBZwFXa"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0cPA5q6wFXa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import layers\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "def preprocess(array):\n",
        "    \"\"\"Normalizes the supplied array and reshapes it.\"\"\"\n",
        "    array = array.astype(\"float32\") / 255.0\n",
        "    array = np.reshape(array, (len(array), 28, 28, 1))\n",
        "    return array\n",
        "\n",
        "\n",
        "def noise(array):\n",
        "    \"\"\"Adds random noise to each image in the supplied array.\"\"\"\n",
        "    noise_factor = 0.4\n",
        "    noisy_array = array + noise_factor * np.random.normal(\n",
        "        loc=0.0, scale=1.0, size=array.shape\n",
        "    )\n",
        "\n",
        "    return np.clip(noisy_array, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def display(array1, array2):\n",
        "    \"\"\"Displays ten random images from each array.\"\"\"\n",
        "    n = 10\n",
        "    indices = np.random.randint(len(array1), size=n)\n",
        "    images1 = array1[indices, :]\n",
        "    images2 = array2[indices, :]\n",
        "\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
        "        ax = plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(image1.reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        plt.imshow(image2.reshape(28, 28))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67eK0d6mwFXc"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLqpGNY2wFXc"
      },
      "outputs": [],
      "source": [
        "# Since we only need images from the dataset to encode and decode, we\n",
        "# won't use the labels.\n",
        "(train_data, _), (test_data, _) = mnist.load_data()\n",
        "\n",
        "# Normalize and reshape the data\n",
        "train_data = preprocess(train_data)\n",
        "test_data = preprocess(test_data)\n",
        "\n",
        "# Create a copy of the data with added noise\n",
        "noisy_train_data = noise(train_data)\n",
        "noisy_test_data = noise(test_data)\n",
        "\n",
        "# Display the train data and a version of it with added noise\n",
        "display(train_data, noisy_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbeHht0wwFXd"
      },
      "source": [
        "## Build the autoencoder\n",
        "\n",
        "We are going to use the Functional API to build our convolutional autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aspYIVgowFXd"
      },
      "outputs": [],
      "source": [
        "input = layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder\n",
        "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input)\n",
        "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
        "\n",
        "# Decoder\n",
        "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "# Autoencoder\n",
        "autoencoder = Model(input, x)\n",
        "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_OYqkeawFXe"
      },
      "source": [
        "Now we can train our autoencoder using `train_data` as both our input data\n",
        "and target. Notice we are setting up the validation data using the same\n",
        "format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLjmZANDwFXe"
      },
      "outputs": [],
      "source": [
        "autoencoder.fit(\n",
        "    x=train_data,\n",
        "    y=train_data,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    validation_data=(test_data, test_data),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8LaZ1riwFXe"
      },
      "source": [
        "Let's predict on our test dataset and display the original image together with\n",
        "the prediction from our autoencoder.\n",
        "\n",
        "Notice how the predictions are pretty close to the original images, although\n",
        "not quite the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UofJhKRwFXe"
      },
      "outputs": [],
      "source": [
        "predictions = autoencoder.predict(test_data)\n",
        "display(test_data, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xXLDXcwwFXf"
      },
      "source": [
        "Now that we know that our autoencoder works, let's retrain it using the noisy\n",
        "data as our input and the clean data as our target. We want our autoencoder to\n",
        "learn how to denoise the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfokHCcNwFXf"
      },
      "outputs": [],
      "source": [
        "autoencoder.fit(\n",
        "    x=noisy_train_data,\n",
        "    y=train_data,\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    validation_data=(noisy_test_data, test_data),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtxFOao3wFXf"
      },
      "source": [
        "Let's now predict on the noisy data and display the results of our autoencoder.\n",
        "\n",
        "Notice how the autoencoder does an amazing job at removing the noise from the\n",
        "input images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HxGPBx2wFXf"
      },
      "outputs": [],
      "source": [
        "predictions = autoencoder.predict(noisy_test_data)\n",
        "display(noisy_test_data, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformers"
      ],
      "metadata": {
        "id": "Kt4V27XsjfkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1. Load a pre-trained sentiment analysis pipeline\n",
        "# This pipeline uses a Transformer model fine-tuned for sentiment analysis.\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# 2. Provide text input\n",
        "text1 = \"I love using the Hugging Face Transformers library!\"\n",
        "text2 = \"This movie was incredibly boring and a waste of time.\"\n",
        "\n",
        "# 3. Perform sentiment analysis\n",
        "result1 = classifier(text1)\n",
        "result2 = classifier(text2)\n",
        "\n",
        "# 4. Print the results\n",
        "print(f\"Text 1: '{text1}'\")\n",
        "print(f\"Sentiment: {result1[0]['label']}, Score: {result1[0]['score']:.4f}\\n\")\n",
        "\n",
        "print(f\"Text 2: '{text2}'\")\n",
        "print(f\"Sentiment: {result2[0]['label']}, Score: {result2[0]['score']:.4f}\")"
      ],
      "metadata": {
        "id": "yV23CCDljl0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "PG_YPNLjkb4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipeline = pipeline(\n",
        "    task=\"fill-mask\",\n",
        "    model=\"google-bert/bert-base-uncased\",\n",
        "    dtype=torch.float16,\n",
        "    device=0\n",
        ")\n",
        "pipeline(\"Plants create [MASK] through a process known as photosynthesis.\")"
      ],
      "metadata": {
        "id": "hl3aJJNAkdxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable Diffusion"
      ],
      "metadata": {
        "id": "i2pC7mDqEgZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras_cv --upgrade --quiet"
      ],
      "metadata": {
        "id": "7IrraSy3Ei8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_cv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Instantiate the Stable Diffusion model\n",
        "# This will automatically download the pre-trained weights\n",
        "model = keras_cv.models.StableDiffusion(jit_compile=True)\n",
        "\n",
        "# Define your text prompt\n",
        "prompt = \"A beautiful horse running through a field, highly detailed, 8k\"\n",
        "\n",
        "# Generate images\n",
        "images = model.text_to_image(prompt, batch_size=3, num_steps=25)\n",
        "\n",
        "# Plot the generated images (optional, for visualization)\n",
        "def plot_images(images):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(images)):\n",
        "        ax = plt.subplot(1, len(images), i + 1)\n",
        "        plt.imshow(images[i].astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "plot_images(images)\n"
      ],
      "metadata": {
        "id": "ownU4MvoEkRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}