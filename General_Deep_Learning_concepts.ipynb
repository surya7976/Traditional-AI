{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOpEkBAOUKNJ"
      },
      "source": [
        "**Tensorflow 1 vs Tensorflow 2** - Understand what changed\n",
        "* One of the most important changes is that keras which was earlier being used by Tensorflow and others like Theano is now a part of Tensorflow.\n",
        "* So the syntax in using Keras changes to Tensorflow.keras....\n",
        "\n",
        "https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/\n",
        "\n",
        "**Migrate Tensoflow 1 to 2**\n",
        "\n",
        "https://www.tensorflow.org/guide/migrate\n",
        "\n",
        "**3 ways of creating Keras models**\n",
        "\n",
        "https://towardsdatascience.com/3-ways-to-build-neural-networks-in-tensorflow-with-the-keras-api-80e92d3b5b7e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R5uHkYe72mU"
      },
      "source": [
        "###Multi-Layer Perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeo2DC4mTL3h"
      },
      "source": [
        "\n",
        "* Nice visualization of MLPs\n",
        "https://www.youtube.com/watch?v=3JQ3hYko51Y\n",
        "\n",
        "* Another one https://www.youtube.com/watch?v=Aut32pR5PQA\n",
        "\n",
        "-----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawPfBnJ8hxS"
      },
      "source": [
        "* The field of artificial neural networks is often just called Neural Networks or Multilayer Perceptrons.  \n",
        "\n",
        "* A **Perceptron** is a single neuron model.\n",
        "\n",
        "**Shallow vs Deep Networks** - Neural netwroks with more than one hidden layer are Deep networks.\n",
        "\n",
        "* Adding more layers allows for more easy representation of the interactions within the input data, as well as allows for more abstract features to be learned and used as input into the next hidden layer.\n",
        "\n",
        "* The predictive capability of neural networks comes from the hierarchical or multilayered structure of the networks. The data structure can learn  features at different scales and combine them into higher-order features.\n",
        "\n",
        "* **Neuron** - The building block for neural networks are artificial neurons. These are simple computational units that have weighted input signals and produce an output signal using an activation function.\n",
        "\n",
        "* **Neuron Weights** - They are similar to weights in Linear regression. A neuron may have two inputs in which case it requires three weights. One for each input and one for the bias.  Weights are often initialized to small random values, such as values in the range 0 to 0.3. Larger weights indicate increased complexity and fragility of the model.\n",
        "\n",
        "-----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2rcS-bUYZjj"
      },
      "source": [
        "**Input or Visible layers**\n",
        "* The bottom layer that takes input from your dataset is called the visible layer, because it is the exposed part of the network. Often a neural network is drawn with a visible layer with one neuron per input value or column in your dataset.\n",
        "\n",
        "**Hidden Layers**\n",
        "* Layers after the input layer are called hidden layers because they are not directly exposed to the input.\n",
        "\n",
        "**Output Layer**\n",
        "* The final hidden layer is called the output layer and it is responsible for outputting a value or vector of values that correspond to the format required for the problem. The choice of activation function in the output layer is strongly constrained by the type of problem that you are modeling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pskot4liCtWs"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZoagwjNDhEI"
      },
      "source": [
        "**Activation functions :**\n",
        "\n",
        "* **A neural network without an activation function is essentially just a linear regression model.**\n",
        "\n",
        "* An activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. It takes in the output signal from the previous cell and converts it into some form that can be taken as input to the next cell.\n",
        "\n",
        "* An activation function is a simple mapping of summed weighted input to the output of the neuron.\n",
        "\n",
        "* Traditionally nonlinear activation functions are used. This allows the network to combine the inputs in more complex ways and in turn provide a richer capability in the functions they can model.\n",
        "\n",
        "* They also help in keeping the value of the output from the neuron restricted to a certain limit as per our requirement.\n",
        "\n",
        "* Activation functions are applied at every layer and need to be calculated many times in deep networks. Hence, they should be computationally inexpensive to calculate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcT8IgqGJptC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5isgngZIJQBC"
      },
      "source": [
        "* Logistic function also called the **sigmoid** function is used to output a value between 0 and 1. Useful for binary classification.\n",
        "\n",
        "* The **softmax** is a more generalised form of the sigmoid. It is used in multi-class classification problems. Similar to sigmoid, it produces values in the range of 0–1 therefore it is used as the final layer in classification models. Useful for multiclass-classification.\n",
        "\n",
        "* The hyperbolic tangent function also called **Tanh** outputs the same distribution over the range -1 to +1.  The tanh function is very similar to the sigmoid function. The only difference is that it is symmetric around the origin.  \n",
        "\n",
        "* It is usually used in hidden layers of a neural network as it’s values lies between -1 to 1 hence the mean for the hidden layer comes out be 0 or very close to it, hence helps in centering the data by bringing mean close to 0. This makes learning for the next layer much easier.\n",
        "\n",
        "* The rectifier activation function **ReLU** (Rectified Linear Unit) outputs 0 fro -ve values and the actual value for +ve values\n",
        "\n",
        "* The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.  Since only a certain number of neurons are activated, the ReLU function is far more computationally efficient.\n",
        "\n",
        "* **Leaky ReLU** function is nothing but an improved version of the ReLU function. As we saw that for the ReLU function, the gradient is 0 for x<0, which would deactivate the neurons in that region.\n",
        "\n",
        "* Leaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for negative values of x, we define it as an extremely small linear component of x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJgdcCQGCwwr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXfkPJNZDTrQ"
      },
      "source": [
        "**When to use which Activation funtion?**\n",
        "\n",
        "* Sigmoid functions and their combinations generally work better in the case of classifiers\n",
        "\n",
        "* ReLU function is a general activation function and is used in most cases these days\n",
        "\n",
        "* If we encounter a case of dead neurons in our networks the leaky ReLU function is the best choice\n",
        "\n",
        "* Always keep in mind that ReLU function should only be used in the hidden layers\n",
        "\n",
        "* As a rule of thumb, you can begin with using ReLU function and then move over to other activation functions in case ReLU doesn’t provide with optimum results\n",
        "\n",
        "* No activation function is used for the output layer if it is a regression problem as we are interested in predicting numerical values directly without transform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t0Hl0A0eOCm"
      },
      "source": [
        "**Epochs**\n",
        "\n",
        "* An epoch is one learning cycle where the learner sees the whole training data set. If you have two batches, the learner needs to go through two iterations for one epoch\n",
        "\n",
        "* The number of epochs needs to be finetuned based on the validation and training error. As long as they keep dropping training should continue.\n",
        "\n",
        "* For instance, if the validation error starts increasing that might be a indication of overfitting. You should set the number of epochs as high as possible and terminate training based on the error rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzjqB6TbrfTk"
      },
      "source": [
        "**Backpropagation**\n",
        "\n",
        "* Backpropagation is the central mechanism by which neural networks learn. It is probably the most fundamental building block in a neural network.\n",
        "\n",
        "* The algorithm is used to effectively train a neural network through a method called chain rule. In simple terms, after each forward pass through a network, backpropagation performs a backward pass while adjusting the model’s parameters (weights and biases).\n",
        "\n",
        "* The final step in a forward pass is to evaluate the predicted output against an expected output. It happens through a **cost / loss function**. This can be as simple as MSE (mean squared error) or more complex like cross-entropy.\n",
        "\n",
        "* In other words, backpropagation aims to minimize the cost function by adjusting network’s weights and biases. The level of adjustment is determined by the gradients of the cost function with respect to those parameters.\n",
        "\n",
        "* It uses the Gradient descent algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGzPMoMcreGJ"
      },
      "source": [
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdSPRTjB4Wq0"
      },
      "source": [
        "**Cost / Loss Functions**\n",
        "\n",
        "* Loss functions are helpful to train a neural network. Given an input and a target, they calculate the loss, i.e difference between output and target variable.\n",
        "\n",
        "* A loss function is for a single training example. It is also sometimes called an error function. A cost function, on the other hand, is the average loss over the entire training dataset. The optimization strategies aim at minimizing the cost function.\n",
        "\n",
        "* **Optimizers** are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses - Explained in-depth later\n",
        "\n",
        "**Different Loss functions**\n",
        "\n",
        "* For Regression - **MSE** (Mean Square Error)\n",
        "\n",
        "* Mean squared error is calculated as the average of the squared differences between the predicted and actual values. The result is always positive. The squaring means that larger mistakes result in more error than smaller mistakes, meaning that the model is punished for making larger mistakes.\n",
        "\n",
        "* For Classification - **Cross-entropy** will calculate a score that summarizes the average difference between the actual and predicted probability distributions\n",
        "\n",
        "    * Binary Cross-Entropy\n",
        "    * Sparse Categorical Cross-Entropy\n",
        "\n",
        "* Know more at https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcJfjO8qTNtP"
      },
      "source": [
        "**Binary Classification with MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_prS8AdZmtG",
        "outputId": "7a783a19-59b6-4247-ec68-12f17119a488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "# https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
        "\n",
        "import pandas as pd\n",
        "import numpy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv')\n",
        "\n",
        "y = dataset['Outcome']\n",
        "X = dataset.drop(['Outcome'], axis = 1)\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "# The first layer has 8 nodes and uses the relu activation function.\n",
        "# The line of code that adds the first Dense layer is doing 2 things,\n",
        "# defining the input or visible layer and the first hidden layer\n",
        "\n",
        "#The implicit input layer is the reason why you have to include an\n",
        "#input_dim argument only in the first (explicit) layer of the model\n",
        "#in the Sequential API - in subsequent layers, the input shape\n",
        "#is inferred from the output of the previous ones\n",
        "\n",
        "# first layer\n",
        "model.add(Dense(8, input_dim=8, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "#model.add(Dense(64, activation='relu'))\n",
        "# output layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 9 (8+1)*8\n",
        "# 9 (8+1)*16\n",
        "# 17 (16+1)*32\n",
        "# 33 (32+1)*1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_17\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_17\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_69 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m72\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_70 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m144\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_71 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m544\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_72 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m793\u001b[0m (3.10 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">793</span> (3.10 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m793\u001b[0m (3.10 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">793</span> (3.10 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRBPZGMGuh2R",
        "outputId": "e086f880-9267-435d-8558-a07cc79d7deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "numpy.random.seed(7)\n",
        "# verbose=0 will show you nothing (silent)\n",
        "# verbose=1 will show you an animated progress bar\n",
        "# verbose=2 will just mention the number of epochs\n",
        "\n",
        "# batch size determines how many rows should be taken at a time.\n",
        "# default is 32 etc. based on loss function\n",
        "hist=model.fit(X_train_scaled, y_train, epochs=20, verbose=1)\n",
        "# increase no of epochs and see\n",
        "\n",
        "# history attribute is a dictionary recording training loss values\n",
        "# and metrics values at successive epochs, as well as validation\n",
        "# loss values and validation metrics values (if applicable).\n",
        "print(hist.history)\n",
        "\n",
        "score = model.evaluate(X_test_scaled, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# save model with weights if required\n",
        "model.save(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6356 - loss: 0.6919\n",
            "Epoch 2/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6449 - loss: 0.6822 \n",
            "Epoch 3/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6454 - loss: 0.6683 \n",
            "Epoch 4/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6647 - loss: 0.6524 \n",
            "Epoch 5/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6550 - loss: 0.6305 \n",
            "Epoch 6/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6548 - loss: 0.6270 \n",
            "Epoch 7/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7078 - loss: 0.5984 \n",
            "Epoch 8/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7415 - loss: 0.5662 \n",
            "Epoch 9/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7558 - loss: 0.5431 \n",
            "Epoch 10/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7591 - loss: 0.5141 \n",
            "Epoch 11/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7646 - loss: 0.5105 \n",
            "Epoch 12/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7587 - loss: 0.4813 \n",
            "Epoch 13/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8025 - loss: 0.4502 \n",
            "Epoch 14/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7751 - loss: 0.4702 \n",
            "Epoch 15/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7857 - loss: 0.4544 \n",
            "Epoch 16/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7851 - loss: 0.4551 \n",
            "Epoch 17/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7872 - loss: 0.4431 \n",
            "Epoch 18/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7775 - loss: 0.4555 \n",
            "Epoch 19/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8028 - loss: 0.4312 \n",
            "Epoch 20/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7981 - loss: 0.4365 \n",
            "{'accuracy': [0.6336805820465088, 0.6579861044883728, 0.6614583134651184, 0.6614583134651184, 0.6614583134651184, 0.6805555820465088, 0.7274305820465088, 0.75, 0.7569444179534912, 0.7621527910232544, 0.765625, 0.765625, 0.7690972089767456, 0.7743055820465088, 0.7847222089767456, 0.7829861044883728, 0.7847222089767456, 0.7899305820465088, 0.7916666865348816, 0.7934027910232544], 'loss': [0.693915069103241, 0.6757147312164307, 0.6612067222595215, 0.6466668844223022, 0.6316981315612793, 0.6119775176048279, 0.587308406829834, 0.5614225268363953, 0.5370752811431885, 0.51494961977005, 0.4999728202819824, 0.48657718300819397, 0.47824499011039734, 0.47227784991264343, 0.4671946167945862, 0.46173620223999023, 0.4590032398700714, 0.45502397418022156, 0.45405590534210205, 0.45010560750961304]}\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7679 - loss: 0.5262  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.5482065677642822\n",
            "Test accuracy: 0.7447916865348816\n",
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOZLJiwg-Ve-"
      },
      "source": [
        "**Saving a model along with wieghts can save lot of time especally if it very complex**\n",
        "\n",
        "* Keras supports saving a single HDF5 file containing the model's architecture, weights values, and compile() information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fzR4g6X9wuE",
        "outputId": "8a1de369-f253-4b20-b383-ebe65d002899",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "# load model\n",
        "new_model = load_model('model.h5')\n",
        "\n",
        "score = new_model.evaluate(X_test_scaled, y_test, batch_size=40)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "y_pred=new_model.predict(X_test_scaled)\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7698 - loss: 0.5212  \n",
            "Test loss: 0.548206627368927\n",
            "Test accuracy: 0.7447916865348816\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "[[0.08720635]\n",
            " [0.865073  ]\n",
            " [0.80952615]\n",
            " [0.11440851]\n",
            " [0.3977881 ]\n",
            " [0.5574949 ]\n",
            " [0.08730539]\n",
            " [0.1751891 ]\n",
            " [0.960117  ]\n",
            " [0.34492612]\n",
            " [0.73548144]\n",
            " [0.14194585]\n",
            " [0.8853268 ]\n",
            " [0.8964774 ]\n",
            " [0.20786686]\n",
            " [0.22731636]\n",
            " [0.18474075]\n",
            " [0.10875557]\n",
            " [0.18843195]\n",
            " [0.12294129]\n",
            " [0.38947842]\n",
            " [0.33782881]\n",
            " [0.76478237]\n",
            " [0.392474  ]\n",
            " [0.11148787]\n",
            " [0.09903792]\n",
            " [0.09707835]\n",
            " [0.48980975]\n",
            " [0.28607917]\n",
            " [0.61219984]\n",
            " [0.38756666]\n",
            " [0.24353585]\n",
            " [0.15402843]\n",
            " [0.61525327]\n",
            " [0.09463776]\n",
            " [0.23220173]\n",
            " [0.3954927 ]\n",
            " [0.09017936]\n",
            " [0.6746777 ]\n",
            " [0.6207653 ]\n",
            " [0.6517605 ]\n",
            " [0.8323549 ]\n",
            " [0.14675389]\n",
            " [0.04820412]\n",
            " [0.11335704]\n",
            " [0.10270902]\n",
            " [0.581502  ]\n",
            " [0.2181901 ]\n",
            " [0.1903658 ]\n",
            " [0.93463767]\n",
            " [0.36470225]\n",
            " [0.05164205]\n",
            " [0.3662571 ]\n",
            " [0.03417829]\n",
            " [0.46105734]\n",
            " [0.36143458]\n",
            " [0.09273617]\n",
            " [0.0962282 ]\n",
            " [0.20248044]\n",
            " [0.18609202]\n",
            " [0.11516564]\n",
            " [0.78562415]\n",
            " [0.14659026]\n",
            " [0.5896837 ]\n",
            " [0.6702551 ]\n",
            " [0.60557956]\n",
            " [0.33373243]\n",
            " [0.7248957 ]\n",
            " [0.8979236 ]\n",
            " [0.44815227]\n",
            " [0.9024251 ]\n",
            " [0.8154175 ]\n",
            " [0.16901395]\n",
            " [0.22518414]\n",
            " [0.50941515]\n",
            " [0.31992564]\n",
            " [0.2638516 ]\n",
            " [0.13810422]\n",
            " [0.04700615]\n",
            " [0.11856018]\n",
            " [0.19086912]\n",
            " [0.3008229 ]\n",
            " [0.6815334 ]\n",
            " [0.04724889]\n",
            " [0.13279231]\n",
            " [0.28115553]\n",
            " [0.93532044]\n",
            " [0.00966576]\n",
            " [0.60787696]\n",
            " [0.12832639]\n",
            " [0.54765904]\n",
            " [0.41873637]\n",
            " [0.47042483]\n",
            " [0.5409608 ]\n",
            " [0.4606424 ]\n",
            " [0.39533418]\n",
            " [0.8316611 ]\n",
            " [0.05925535]\n",
            " [0.45023394]\n",
            " [0.8138909 ]\n",
            " [0.49829066]\n",
            " [0.153453  ]\n",
            " [0.20475024]\n",
            " [0.5245359 ]\n",
            " [0.06514997]\n",
            " [0.26640123]\n",
            " [0.1617834 ]\n",
            " [0.4168274 ]\n",
            " [0.6185305 ]\n",
            " [0.8316463 ]\n",
            " [0.09940038]\n",
            " [0.71507305]\n",
            " [0.07662444]\n",
            " [0.12975815]\n",
            " [0.8356565 ]\n",
            " [0.28308743]\n",
            " [0.05369986]\n",
            " [0.14706875]\n",
            " [0.40803605]\n",
            " [0.79181576]\n",
            " [0.13295849]\n",
            " [0.01689463]\n",
            " [0.06961644]\n",
            " [0.05666313]\n",
            " [0.05395506]\n",
            " [0.8487318 ]\n",
            " [0.06901497]\n",
            " [0.5110585 ]\n",
            " [0.15143947]\n",
            " [0.2147271 ]\n",
            " [0.07299341]\n",
            " [0.97257173]\n",
            " [0.07555757]\n",
            " [0.10216738]\n",
            " [0.2343445 ]\n",
            " [0.05923829]\n",
            " [0.08212481]\n",
            " [0.3471205 ]\n",
            " [0.05797614]\n",
            " [0.99302113]\n",
            " [0.24589418]\n",
            " [0.20767865]\n",
            " [0.30577114]\n",
            " [0.6362541 ]\n",
            " [0.11512429]\n",
            " [0.75152135]\n",
            " [0.31721506]\n",
            " [0.14795524]\n",
            " [0.89699244]\n",
            " [0.71598643]\n",
            " [0.23144239]\n",
            " [0.45905247]\n",
            " [0.15506603]\n",
            " [0.07197519]\n",
            " [0.2648064 ]\n",
            " [0.17542785]\n",
            " [0.65268004]\n",
            " [0.10176486]\n",
            " [0.18341878]\n",
            " [0.10802948]\n",
            " [0.28948388]\n",
            " [0.12690121]\n",
            " [0.29060188]\n",
            " [0.06499846]\n",
            " [0.7919627 ]\n",
            " [0.22710705]\n",
            " [0.49493542]\n",
            " [0.06664489]\n",
            " [0.17440969]\n",
            " [0.91429263]\n",
            " [0.09692544]\n",
            " [0.61183935]\n",
            " [0.77627844]\n",
            " [0.31324643]\n",
            " [0.2749038 ]\n",
            " [0.3791353 ]\n",
            " [0.6298016 ]\n",
            " [0.37056047]\n",
            " [0.53871477]\n",
            " [0.39116335]\n",
            " [0.42032573]\n",
            " [0.16137758]\n",
            " [0.07632019]\n",
            " [0.07978277]\n",
            " [0.2716115 ]\n",
            " [0.24752721]\n",
            " [0.07209261]\n",
            " [0.10647083]\n",
            " [0.06780332]\n",
            " [0.84922445]\n",
            " [0.61675525]\n",
            " [0.2710528 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibaYdnmZhAx"
      },
      "source": [
        "**Multi-Classification with MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrBtpUQk_UT_",
        "outputId": "d94f4cb5-87b4-4f86-8bb7-04907c4c66cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Plot ad hoc mnist instances\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "# load the MNIST dataset. load only if reqired\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(X_test[1])\n",
        "print(y_test[1])\n",
        "# plot image as gray scale\n",
        "plt.imshow(X_test[1], cmap=plt.get_cmap('gray'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 116 125 171 255 255 150  93   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0 169 253 253 253 253 253 253 218  30\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 169 253 253 253 213 142 176 253 253 122\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  52 250 253 210  32  12   0   6 206 253 140\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  77 251 210  25   0   0   0 122 248 253  65\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  31  18   0   0   0   0 209 253 253  65\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 117 247 253 198  10\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  76 247 253 231  63   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0 128 253 253 144   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 176 246 253 159  12   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  25 234 253 233  35   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 198 253 253 141   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  78 248 253 189  12   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  19 200 253 253 141   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 134 253 253 173  12   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 248 253 253  25   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 248 253 253  43  20  20  20  20   5   0\n",
            "    5  20  20  37 150 150 150 147  10   0]\n",
            " [  0   0   0   0   0   0   0   0 248 253 253 253 253 253 253 253 168 143\n",
            "  166 253 253 253 253 253 253 253 123   0]\n",
            " [  0   0   0   0   0   0   0   0 174 253 253 253 253 253 253 253 253 253\n",
            "  253 253 249 247 247 169 117 117  57   0]\n",
            " [  0   0   0   0   0   0   0   0   0 118 123 123 123 166 253 253 253 155\n",
            "  123 123  41   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGydJREFUeJzt3Xts1fX9x/HXAdojantYqe1p5WIBlU2ki1y6DmU4Gkq3ISBbwPkHLkYDK2ZSLqZGrTKXbizZjAvD/bHBmHKRKDDdgtFqyy4tBpQQt9HQpkoNbRksnNMWW1j7+f3BzzOPtOD3cE7fvTwfySeh53w/PW+/O+G5b8/h1OeccwIAoI8Nsx4AADA0ESAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGBihPUAn9fd3a2TJ08qJSVFPp/PehwAgEfOObW2tio7O1vDhvV+ndPvAnTy5EmNHTvWegwAwFVqbGzUmDFjer2/3/0ILiUlxXoEAEAcXOnv84QFaNOmTbrpppt0zTXXKC8vT+++++4X2seP3QBgcLjS3+cJCdCuXbtUUlKisrIyvffee8rNzVVhYaFOnTqViIcDAAxELgFmzpzpiouLI193dXW57OxsV15efsW9oVDISWKxWCzWAF+hUOiyf9/H/Qro/PnzOnz4sAoKCiK3DRs2TAUFBaqurr7k+M7OToXD4agFABj84h6g06dPq6urS5mZmVG3Z2Zmqrm5+ZLjy8vLFQgEIot3wAHA0GD+LrjS0lKFQqHIamxstB4JANAH4v7vgNLT0zV8+HC1tLRE3d7S0qJgMHjJ8X6/X36/P95jAAD6ubhfASUnJ2vatGmqqKiI3Nbd3a2Kigrl5+fH++EAAANUQj4JoaSkRMuXL9f06dM1c+ZMPffcc2pvb9cPfvCDRDwcAGAASkiAli5dqn//+9966qmn1NzcrK9+9avav3//JW9MAAAMXT7nnLMe4rPC4bACgYD1GACAqxQKhZSamtrr/ebvggMADE0ECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgYoT1AMCVrF271vOekSNHxvRYU6dO9bznu9/9bkyP5dXmzZs976muro7psf7whz/EtA/wgisgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCEzznnrIf4rHA4rEAgYD0GEmTXrl2e9/TVh30ORvX19THtKygo8LznxIkTMT0WBq9QKKTU1NRe7+cKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwMcJ6AAxcg/GDRY8dO+Z5zxtvvOF5z4QJEzzvWbBggec9EydO9LxHku6//37Pe8rLy2N6LAxdXAEBAEwQIACAibgH6Omnn5bP54takydPjvfDAAAGuIS8BnTbbbfprbfe+t+DjOClJgBAtISUYcSIEQoGg4n41gCAQSIhrwEdP35c2dnZmjBhgu6///7L/qrezs5OhcPhqAUAGPziHqC8vDxt3bpV+/fv1+bNm9XQ0KC77rpLra2tPR5fXl6uQCAQWWPHjo33SACAfijuASoqKtL3vvc9TZ06VYWFhfrzn/+ss2fP6uWXX+7x+NLSUoVCochqbGyM90gAgH4o4e8OGDVqlG655RbV1dX1eL/f75ff70/0GACAfibh/w6ora1N9fX1ysrKSvRDAQAGkLgHaO3ataqqqtKHH36ov//971q8eLGGDx+u++67L94PBQAYwOL+I7iPP/5Y9913n86cOaMbbrhBd955p2pqanTDDTfE+6EAAANY3AO0c+fOeH9LJNj06dNj2rd48eI4T9Kzf/zjH5733HPPPTE91unTpz3vaWtr87wnOTnZ856amhrPe3Jzcz3vkaTRo0fHtA/wgs+CAwCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMJPwX0qH/i/V3Nfl8Ps97Yvlg0cLCQs97mpqaPO/pS2vWrPG85ytf+UoCJunZn/70pz57LAxdXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABJ+GDb322msx7Zs0aZLnPa2trZ73/Oc///G8p79btmyZ5z1JSUkJmASwwxUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCDyNFzD766CPrEfqFdevWed5zyy23JGCSSx08eLBP9wFecAUEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgw0iBz/jOd77jec+GDRs870lOTva859SpU573lJaWet4jSefOnYtpH+AFV0AAABMECABgwnOADhw4oAULFig7O1s+n0979+6Nut85p6eeekpZWVkaOXKkCgoKdPz48XjNCwAYJDwHqL29Xbm5udq0aVOP92/cuFHPP/+8XnjhBR08eFDXXXedCgsL1dHRcdXDAgAGD89vQigqKlJRUVGP9znn9Nxzz+mJJ57QwoULJUnbtm1TZmam9u7dq2XLll3dtACAQSOurwE1NDSoublZBQUFkdsCgYDy8vJUXV3d457Ozk6Fw+GoBQAY/OIaoObmZklSZmZm1O2ZmZmR+z6vvLxcgUAgssaOHRvPkQAA/ZT5u+BKS0sVCoUiq7Gx0XokAEAfiGuAgsGgJKmlpSXq9paWlsh9n+f3+5Wamhq1AACDX1wDlJOTo2AwqIqKisht4XBYBw8eVH5+fjwfCgAwwHl+F1xbW5vq6uoiXzc0NOjIkSNKS0vTuHHj9Oijj+rZZ5/VzTffrJycHD355JPKzs7WokWL4jk3AGCA8xygQ4cO6e677458XVJSIklavny5tm7dqvXr16u9vV0PP/ywzp49qzvvvFP79+/XNddcE7+pAQADnucAzZkzR865Xu/3+XzasGFDTB/QCFibPn265z2xfLBoLHbt2uV5T1VVVQImAeLD/F1wAIChiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACY8fxo2MBDs3bs3pn3z5s2L7yC92LZtm+c9TzzxRAImAexwBQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmODDSNHvZWVled7z9a9/PabH8vv9nvecPn3a855nn33W8562tjbPe4D+jCsgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEH0aKfu+VV17xvGf06NEJmKRnL774ouc99fX1CZgEGFi4AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPBhpOhT99xzj+c9d9xxRwIm6VllZaXnPWVlZfEfBBgCuAICAJggQAAAE54DdODAAS1YsEDZ2dny+Xzau3dv1P0PPPCAfD5f1Jo/f3685gUADBKeA9Te3q7c3Fxt2rSp12Pmz5+vpqamyNqxY8dVDQkAGHw8vwmhqKhIRUVFlz3G7/crGAzGPBQAYPBLyGtAlZWVysjI0K233qqVK1fqzJkzvR7b2dmpcDgctQAAg1/cAzR//nxt27ZNFRUV+tnPfqaqqioVFRWpq6urx+PLy8sVCAQia+zYsfEeCQDQD8X93wEtW7Ys8ufbb79dU6dO1cSJE1VZWam5c+decnxpaalKSkoiX4fDYSIEAENAwt+GPWHCBKWnp6uurq7H+/1+v1JTU6MWAGDwS3iAPv74Y505c0ZZWVmJfigAwADi+UdwbW1tUVczDQ0NOnLkiNLS0pSWlqZnnnlGS5YsUTAYVH19vdavX69JkyapsLAwroMDAAY2zwE6dOiQ7r777sjXn75+s3z5cm3evFlHjx7V73//e509e1bZ2dmaN2+efvzjH8vv98dvagDAgOc5QHPmzJFzrtf733jjjasaCAPH6NGjPe95/PHHPe9JSkryvCdWR44c8bynra0t/oMAQwCfBQcAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATcf+V3Bg61qxZ43nPjBkzEjDJpfbu3RvTvrKysvgOAqBXXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZ8zjlnPcRnhcNhBQIB6zHwBXR0dHjek5SUlIBJLjVmzJiY9jU1NcV5EmDoCoVCSk1N7fV+roAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMjrAcAEiEtLS2mfRcuXIjzJLZCoVBM+2I5D7F80GxfffDwqFGjYtpXUlIS30HiqKurK6Z9jz32mOc9586di+mxroQrIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABB9GikHp6NGj1iP0C7t3745pX1NTk+c9mZmZnvcsXbrU8x5cnebmZs97fvKTnyRgEq6AAABGCBAAwISnAJWXl2vGjBlKSUlRRkaGFi1apNra2qhjOjo6VFxcrNGjR+v666/XkiVL1NLSEtehAQADn6cAVVVVqbi4WDU1NXrzzTd14cIFzZs3T+3t7ZFjVq9erddee027d+9WVVWVTp48qXvvvTfugwMABjZPb0LYv39/1Ndbt25VRkaGDh8+rNmzZysUCum3v/2ttm/frm9+85uSpC1btujLX/6yampq9LWvfS1+kwMABrSreg3o01/3++mvPz58+LAuXLiggoKCyDGTJ0/WuHHjVF1d3eP36OzsVDgcjloAgMEv5gB1d3fr0Ucf1axZszRlyhRJF9/el5ycfMnvX8/MzOz1rX/l5eUKBAKRNXbs2FhHAgAMIDEHqLi4WB988IF27tx5VQOUlpYqFApFVmNj41V9PwDAwBDTP0RdtWqVXn/9dR04cEBjxoyJ3B4MBnX+/HmdPXs26iqopaVFwWCwx+/l9/vl9/tjGQMAMIB5ugJyzmnVqlXas2eP3n77beXk5ETdP23aNCUlJamioiJyW21trU6cOKH8/Pz4TAwAGBQ8XQEVFxdr+/bt2rdvn1JSUiKv6wQCAY0cOVKBQEAPPvigSkpKlJaWptTUVD3yyCPKz8/nHXAAgCieArR582ZJ0pw5c6Ju37Jlix544AFJ0i9/+UsNGzZMS5YsUWdnpwoLC/XrX/86LsMCAAYPn3POWQ/xWeFwWIFAwHoMfAGvvvqq5z0LFy5MwCQYSv773/963tPd3Z2ASXr2xz/+0fOeQ4cOJWCSnv3lL3/xvKempiamxwqFQkpNTe31fj4LDgBgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb4NGz0qfXr13vek5SUlIBJ4ue2227zvGfp0qUJmCR+fve733ne8+GHH8Z/kB688sornvccO3YsAZPgSvg0bABAv0SAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmODDSAEACcGHkQIA+iUCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAhKcAlZeXa8aMGUpJSVFGRoYWLVqk2traqGPmzJkjn88XtVasWBHXoQEAA5+nAFVVVam4uFg1NTV68803deHCBc2bN0/t7e1Rxz300ENqamqKrI0bN8Z1aADAwDfCy8H79++P+nrr1q3KyMjQ4cOHNXv27Mjt1157rYLBYHwmBAAMSlf1GlAoFJIkpaWlRd3+0ksvKT09XVOmTFFpaanOnTvX6/fo7OxUOByOWgCAIcDFqKury3372992s2bNirr9N7/5jdu/f787evSoe/HFF92NN97oFi9e3Ov3KSsrc5JYLBaLNchWKBS6bEdiDtCKFSvc+PHjXWNj42WPq6iocJJcXV1dj/d3dHS4UCgUWY2NjeYnjcVisVhXv64UIE+vAX1q1apVev3113XgwAGNGTPmssfm5eVJkurq6jRx4sRL7vf7/fL7/bGMAQAYwDwFyDmnRx55RHv27FFlZaVycnKuuOfIkSOSpKysrJgGBAAMTp4CVFxcrO3bt2vfvn1KSUlRc3OzJCkQCGjkyJGqr6/X9u3b9a1vfUujR4/W0aNHtXr1as2ePVtTp05NyH8AAGCA8vK6j3r5Od+WLVucc86dOHHCzZ4926WlpTm/3+8mTZrk1q1bd8WfA35WKBQy/7kli8Visa5+Xenvft//h6XfCIfDCgQC1mMAAK5SKBRSampqr/fzWXAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABP9LkDOOesRAABxcKW/z/tdgFpbW61HAADEwZX+Pve5fnbJ0d3drZMnTyolJUU+ny/qvnA4rLFjx6qxsVGpqalGE9rjPFzEebiI83AR5+Gi/nAenHNqbW1Vdna2hg3r/TpnRB/O9IUMGzZMY8aMuewxqampQ/oJ9inOw0Wch4s4DxdxHi6yPg+BQOCKx/S7H8EBAIYGAgQAMDGgAuT3+1VWVia/3289iinOw0Wch4s4DxdxHi4aSOeh370JAQAwNAyoKyAAwOBBgAAAJggQAMAEAQIAmBgwAdq0aZNuuukmXXPNNcrLy9O7775rPVKfe/rpp+Xz+aLW5MmTrcdKuAMHDmjBggXKzs6Wz+fT3r17o+53zumpp55SVlaWRo4cqYKCAh0/ftxm2AS60nl44IEHLnl+zJ8/32bYBCkvL9eMGTOUkpKijIwMLVq0SLW1tVHHdHR0qLi4WKNHj9b111+vJUuWqKWlxWjixPgi52HOnDmXPB9WrFhhNHHPBkSAdu3apZKSEpWVlem9995Tbm6uCgsLderUKevR+txtt92mpqamyPrrX/9qPVLCtbe3Kzc3V5s2berx/o0bN+r555/XCy+8oIMHD+q6665TYWGhOjo6+njSxLrSeZCk+fPnRz0/duzY0YcTJl5VVZWKi4tVU1OjN998UxcuXNC8efPU3t4eOWb16tV67bXXtHv3blVVVenkyZO69957DaeOvy9yHiTpoYceino+bNy40WjiXrgBYObMma64uDjydVdXl8vOznbl5eWGU/W9srIyl5ubaz2GKUluz549ka+7u7tdMBh0P//5zyO3nT171vn9frdjxw6DCfvG58+Dc84tX77cLVy40GQeK6dOnXKSXFVVlXPu4v/2SUlJbvfu3ZFj/vWvfzlJrrq62mrMhPv8eXDOuW984xvuRz/6kd1QX0C/vwI6f/68Dh8+rIKCgshtw4YNU0FBgaqrqw0ns3H8+HFlZ2drwoQJuv/++3XixAnrkUw1NDSoubk56vkRCASUl5c3JJ8flZWVysjI0K233qqVK1fqzJkz1iMlVCgUkiSlpaVJkg4fPqwLFy5EPR8mT56scePGDernw+fPw6deeuklpaena8qUKSotLdW5c+csxutVv/sw0s87ffq0urq6lJmZGXV7Zmamjh07ZjSVjby8PG3dulW33nqrmpqa9Mwzz+iuu+7SBx98oJSUFOvxTDQ3N0tSj8+PT+8bKubPn697771XOTk5qq+v1+OPP66ioiJVV1dr+PDh1uPFXXd3tx599FHNmjVLU6ZMkXTx+ZCcnKxRo0ZFHTuYnw89nQdJ+v73v6/x48crOztbR48e1WOPPaba2lq9+uqrhtNG6/cBwv8UFRVF/jx16lTl5eVp/Pjxevnll/Xggw8aTob+YNmyZZE/33777Zo6daomTpyoyspKzZ0713CyxCguLtYHH3wwJF4HvZzezsPDDz8c+fPtt9+urKwszZ07V/X19Zo4cWJfj9mjfv8juPT0dA0fPvySd7G0tLQoGAwaTdU/jBo1Srfccovq6uqsRzHz6XOA58elJkyYoPT09EH5/Fi1apVef/11vfPOO1G/viUYDOr8+fM6e/Zs1PGD9fnQ23noSV5eniT1q+dDvw9QcnKypk2bpoqKisht3d3dqqioUH5+vuFk9tra2lRfX6+srCzrUczk5OQoGAxGPT/C4bAOHjw45J8fH3/8sc6cOTOonh/OOa1atUp79uzR22+/rZycnKj7p02bpqSkpKjnQ21trU6cODGong9XOg89OXLkiCT1r+eD9bsgvoidO3c6v9/vtm7d6v75z3+6hx9+2I0aNco1Nzdbj9an1qxZ4yorK11DQ4P729/+5goKClx6ero7deqU9WgJ1dra6t5//333/vvvO0nuF7/4hXv//ffdRx995Jxz7qc//akbNWqU27dvnzt69KhbuHChy8nJcZ988onx5PF1ufPQ2trq1q5d66qrq11DQ4N766233B133OFuvvlm19HRYT163KxcudIFAgFXWVnpmpqaIuvcuXORY1asWOHGjRvn3n77bXfo0CGXn5/v8vPzDaeOvyudh7q6OrdhwwZ36NAh19DQ4Pbt2+cmTJjgZs+ebTx5tAERIOec+9WvfuXGjRvnkpOT3cyZM11NTY31SH1u6dKlLisryyUnJ7sbb7zRLV261NXV1VmPlXDvvPOOk3TJWr58uXPu4luxn3zySZeZmen8fr+bO3euq62ttR06AS53Hs6dO+fmzZvnbrjhBpeUlOTGjx/vHnrooUH3f9J6+u+X5LZs2RI55pNPPnE//OEP3Ze+9CV37bXXusWLF7umpia7oRPgSufhxIkTbvbs2S4tLc35/X43adIkt27dOhcKhWwH/xx+HQMAwES/fw0IADA4ESAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAm/g/p6pwiu6Z9fQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlwqe_iE_UUE",
        "outputId": "10414937-d369-418c-b9c9-8d96392a482f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Baseline MLP for MNIST dataset\n",
        "import numpy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "numpy.random.seed(7)\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# flatten 28*28 images to a 784 vector for each image\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype( 'float32' )\n",
        "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype( 'float32' )\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "# Using the method to_categorical(), a numpy array (or) a vector\n",
        "# which has integers that represent different categories, can be\n",
        "# converted into a numpy array (or) a matrix which has binary values\n",
        "# and has columns equal to the number of categories in the data.\n",
        "\n",
        "y_train = utils.to_categorical(y_train)\n",
        "y_test = utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "print(y_test.shape)\n",
        "print(y_test[1])\n",
        "\n",
        "def baseline_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_pixels, input_dim=num_pixels, activation= 'relu' ))\n",
        "    model.add(Dense(num_classes, activation= 'softmax' ))\n",
        "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' ,\n",
        "                  metrics=[ 'accuracy' ])\n",
        "    return model\n",
        "\n",
        "model = baseline_model()\n",
        "\n",
        "# change epochs to 1 and observe the accuracy\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10,\n",
        "          batch_size=200,verbose=2)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 10)\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 3s - 10ms/step - accuracy: 0.9217 - loss: 0.2786 - val_accuracy: 0.9594 - val_loss: 0.1339\n",
            "Epoch 2/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9680 - loss: 0.1108 - val_accuracy: 0.9693 - val_loss: 0.1001\n",
            "Epoch 3/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9797 - loss: 0.0716 - val_accuracy: 0.9757 - val_loss: 0.0793\n",
            "Epoch 4/10\n",
            "300/300 - 1s - 3ms/step - accuracy: 0.9853 - loss: 0.0504 - val_accuracy: 0.9796 - val_loss: 0.0656\n",
            "Epoch 5/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9896 - loss: 0.0369 - val_accuracy: 0.9780 - val_loss: 0.0706\n",
            "Epoch 6/10\n",
            "300/300 - 1s - 3ms/step - accuracy: 0.9928 - loss: 0.0267 - val_accuracy: 0.9805 - val_loss: 0.0636\n",
            "Epoch 7/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9951 - loss: 0.0198 - val_accuracy: 0.9800 - val_loss: 0.0622\n",
            "Epoch 8/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9964 - loss: 0.0151 - val_accuracy: 0.9814 - val_loss: 0.0578\n",
            "Epoch 9/10\n",
            "300/300 - 1s - 3ms/step - accuracy: 0.9979 - loss: 0.0109 - val_accuracy: 0.9812 - val_loss: 0.0579\n",
            "Epoch 10/10\n",
            "300/300 - 1s - 4ms/step - accuracy: 0.9977 - loss: 0.0095 - val_accuracy: 0.9777 - val_loss: 0.0707\n",
            "Baseline Error: 2.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFxNj0NB-OLM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u61C1ANs-QaC"
      },
      "source": [
        "**Regression with MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fklLr9f-UGf",
        "outputId": "4e822cff-6775-480d-baf5-3cddd2139b5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_scaled[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
            "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
            "  0.8252202 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59PeUmPO-yVp",
        "outputId": "3b36e34a-49db-457b-a5ff-d0759f363aa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras import models, layers, regularizers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "# kernel_regularizer applies L1 and L2 regularization to the weights to help\n",
        "# prevent overfitting.\n",
        "model.add(layers.Dense(8, activation='relu', input_shape=[X_train.shape[1]],\n",
        "                       kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
        "model.add(layers.Dense(16, activation='relu',\n",
        "                       kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
        "\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['r2_score'])\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 220ms/step - loss: 593.9835 - r2_score: -6.1655 - val_loss: 637.9644 - val_r2_score: -6.5470\n",
            "Epoch 2/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 557.0887 - r2_score: -5.6255 - val_loss: 629.8168 - val_r2_score: -6.4506\n",
            "Epoch 3/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 533.3517 - r2_score: -5.8590 - val_loss: 622.0002 - val_r2_score: -6.3581\n",
            "Epoch 4/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 564.5382 - r2_score: -5.2748 - val_loss: 613.8042 - val_r2_score: -6.2612\n",
            "Epoch 5/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 551.7649 - r2_score: -5.2701 - val_loss: 605.2876 - val_r2_score: -6.1604\n",
            "Epoch 6/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 518.8646 - r2_score: -5.4789 - val_loss: 596.1796 - val_r2_score: -6.0526\n",
            "Epoch 7/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 532.8314 - r2_score: -4.9153 - val_loss: 585.9204 - val_r2_score: -5.9313\n",
            "Epoch 8/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 488.9440 - r2_score: -5.1572 - val_loss: 573.8110 - val_r2_score: -5.7880\n",
            "Epoch 9/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 514.0200 - r2_score: -5.4658 - val_loss: 559.8810 - val_r2_score: -5.6232\n",
            "Epoch 10/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 499.0251 - r2_score: -5.0458 - val_loss: 544.9352 - val_r2_score: -5.4463\n",
            "Epoch 11/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 468.7469 - r2_score: -4.6714 - val_loss: 527.7406 - val_r2_score: -5.2429\n",
            "Epoch 12/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 464.9340 - r2_score: -4.3383 - val_loss: 506.9645 - val_r2_score: -4.9970\n",
            "Epoch 13/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 420.2060 - r2_score: -4.3971 - val_loss: 482.7118 - val_r2_score: -4.7101\n",
            "Epoch 14/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 455.4150 - r2_score: -3.8663 - val_loss: 455.4500 - val_r2_score: -4.3875\n",
            "Epoch 15/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 407.7742 - r2_score: -3.5748 - val_loss: 425.8251 - val_r2_score: -4.0370\n",
            "Epoch 16/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 356.2733 - r2_score: -3.5612 - val_loss: 392.7694 - val_r2_score: -3.6459\n",
            "Epoch 17/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 329.7515 - r2_score: -3.0238 - val_loss: 357.6158 - val_r2_score: -3.2299\n",
            "Epoch 18/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 309.7361 - r2_score: -2.7307 - val_loss: 318.9951 - val_r2_score: -2.7730\n",
            "Epoch 19/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 270.4683 - r2_score: -2.3585 - val_loss: 278.6286 - val_r2_score: -2.2954\n",
            "Epoch 20/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 224.3150 - r2_score: -1.7089 - val_loss: 240.2036 - val_r2_score: -1.8407\n",
            "Epoch 21/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 206.0344 - r2_score: -1.7675 - val_loss: 204.0178 - val_r2_score: -1.4126\n",
            "Epoch 22/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 179.7511 - r2_score: -0.9769 - val_loss: 172.2490 - val_r2_score: -1.0367\n",
            "Epoch 23/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 150.6159 - r2_score: -0.8144 - val_loss: 145.5423 - val_r2_score: -0.7207\n",
            "Epoch 24/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 122.7976 - r2_score: -0.4100 - val_loss: 122.1169 - val_r2_score: -0.4435\n",
            "Epoch 25/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 106.2171 - r2_score: -0.3214 - val_loss: 103.6646 - val_r2_score: -0.2252\n",
            "Epoch 26/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 85.6866 - r2_score: -0.1960 - val_loss: 89.6897 - val_r2_score: -0.0598\n",
            "Epoch 27/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 90.6236 - r2_score: 0.0293 - val_loss: 78.0118 - val_r2_score: 0.0784\n",
            "Epoch 28/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 64.4958 - r2_score: -0.0058 - val_loss: 69.2735 - val_r2_score: 0.1818\n",
            "Epoch 29/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 62.5338 - r2_score: 0.2870 - val_loss: 61.6644 - val_r2_score: 0.2718\n",
            "Epoch 30/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 60.6052 - r2_score: 0.2816 - val_loss: 55.9103 - val_r2_score: 0.3399\n",
            "Epoch 31/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 51.6193 - r2_score: 0.3445 - val_loss: 51.1468 - val_r2_score: 0.3963\n",
            "Epoch 32/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 48.8840 - r2_score: 0.3064 - val_loss: 47.0255 - val_r2_score: 0.4450\n",
            "Epoch 33/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 46.6990 - r2_score: 0.4178 - val_loss: 43.2989 - val_r2_score: 0.4891\n",
            "Epoch 34/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 41.2839 - r2_score: 0.5317 - val_loss: 40.3094 - val_r2_score: 0.5245\n",
            "Epoch 35/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 37.1005 - r2_score: 0.5808 - val_loss: 37.7941 - val_r2_score: 0.5543\n",
            "Epoch 36/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 35.8417 - r2_score: 0.5646 - val_loss: 35.6732 - val_r2_score: 0.5794\n",
            "Epoch 37/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 38.3410 - r2_score: 0.5392 - val_loss: 33.5640 - val_r2_score: 0.6043\n",
            "Epoch 38/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 33.8019 - r2_score: 0.6362 - val_loss: 31.7269 - val_r2_score: 0.6261\n",
            "Epoch 39/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 37.0635 - r2_score: 0.5448 - val_loss: 29.7370 - val_r2_score: 0.6496\n",
            "Epoch 40/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 29.0230 - r2_score: 0.6644 - val_loss: 28.4519 - val_r2_score: 0.6648\n",
            "Epoch 41/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 31.8924 - r2_score: 0.5981 - val_loss: 27.3940 - val_r2_score: 0.6773\n",
            "Epoch 42/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 28.6007 - r2_score: 0.6428 - val_loss: 26.5898 - val_r2_score: 0.6869\n",
            "Epoch 43/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 26.3829 - r2_score: 0.6600 - val_loss: 25.8024 - val_r2_score: 0.6962\n",
            "Epoch 44/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 30.1402 - r2_score: 0.6766 - val_loss: 25.1731 - val_r2_score: 0.7036\n",
            "Epoch 45/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24.6957 - r2_score: 0.7107 - val_loss: 24.6147 - val_r2_score: 0.7102\n",
            "Epoch 46/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 27.8129 - r2_score: 0.6842 - val_loss: 24.0521 - val_r2_score: 0.7169\n",
            "Epoch 47/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 33.3728 - r2_score: 0.6755 - val_loss: 23.5089 - val_r2_score: 0.7233\n",
            "Epoch 48/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24.3055 - r2_score: 0.6709 - val_loss: 23.1053 - val_r2_score: 0.7281\n",
            "Epoch 49/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21.4991 - r2_score: 0.7281 - val_loss: 22.7671 - val_r2_score: 0.7321\n",
            "Epoch 50/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23.1847 - r2_score: 0.7495 - val_loss: 22.5186 - val_r2_score: 0.7350\n",
            "Epoch 51/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25.3880 - r2_score: 0.6941 - val_loss: 22.2428 - val_r2_score: 0.7383\n",
            "Epoch 52/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23.0127 - r2_score: 0.7268 - val_loss: 21.9702 - val_r2_score: 0.7415\n",
            "Epoch 53/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25.4971 - r2_score: 0.7363 - val_loss: 21.5582 - val_r2_score: 0.7464\n",
            "Epoch 54/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21.6809 - r2_score: 0.7478 - val_loss: 21.2714 - val_r2_score: 0.7498\n",
            "Epoch 55/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22.8483 - r2_score: 0.7388 - val_loss: 20.8215 - val_r2_score: 0.7551\n",
            "Epoch 56/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22.7367 - r2_score: 0.7556 - val_loss: 20.6430 - val_r2_score: 0.7572\n",
            "Epoch 57/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22.4738 - r2_score: 0.7201 - val_loss: 20.4611 - val_r2_score: 0.7594\n",
            "Epoch 58/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22.2147 - r2_score: 0.7199 - val_loss: 20.3485 - val_r2_score: 0.7607\n",
            "Epoch 59/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21.9655 - r2_score: 0.7203 - val_loss: 20.4552 - val_r2_score: 0.7595\n",
            "Epoch 60/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 19.2519 - r2_score: 0.7640 - val_loss: 20.3659 - val_r2_score: 0.7605\n",
            "Epoch 61/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22.6020 - r2_score: 0.7435 - val_loss: 20.1475 - val_r2_score: 0.7631\n",
            "Epoch 62/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22.1471 - r2_score: 0.7540 - val_loss: 19.9301 - val_r2_score: 0.7657\n",
            "Epoch 63/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 19.9167 - r2_score: 0.7739 - val_loss: 19.6756 - val_r2_score: 0.7687\n",
            "Epoch 64/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 21.5681 - r2_score: 0.7189 - val_loss: 19.5294 - val_r2_score: 0.7704\n",
            "Epoch 65/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20.3511 - r2_score: 0.7830 - val_loss: 19.4529 - val_r2_score: 0.7713\n",
            "Epoch 66/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20.5741 - r2_score: 0.7501 - val_loss: 19.2870 - val_r2_score: 0.7733\n",
            "Epoch 67/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23.3185 - r2_score: 0.7515 - val_loss: 19.0866 - val_r2_score: 0.7757\n",
            "Epoch 68/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20.9484 - r2_score: 0.7674 - val_loss: 18.9232 - val_r2_score: 0.7776\n",
            "Epoch 69/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 18.7078 - r2_score: 0.7724 - val_loss: 18.7833 - val_r2_score: 0.7792\n",
            "Epoch 70/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20.4374 - r2_score: 0.7742 - val_loss: 18.7129 - val_r2_score: 0.7801\n",
            "Epoch 71/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20.3701 - r2_score: 0.7740 - val_loss: 18.5953 - val_r2_score: 0.7815\n",
            "Epoch 72/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.1498 - r2_score: 0.7956 - val_loss: 18.5793 - val_r2_score: 0.7817\n",
            "Epoch 73/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20.3667 - r2_score: 0.7608 - val_loss: 18.4064 - val_r2_score: 0.7837\n",
            "Epoch 74/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 19.7371 - r2_score: 0.7647 - val_loss: 18.1993 - val_r2_score: 0.7862\n",
            "Epoch 75/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 17.2529 - r2_score: 0.8044 - val_loss: 18.1447 - val_r2_score: 0.7868\n",
            "Epoch 76/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 21.6393 - r2_score: 0.7626 - val_loss: 18.1421 - val_r2_score: 0.7868\n",
            "Epoch 77/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 19.3068 - r2_score: 0.7160 - val_loss: 17.7908 - val_r2_score: 0.7910\n",
            "Epoch 78/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 21.0666 - r2_score: 0.7762 - val_loss: 17.6549 - val_r2_score: 0.7926\n",
            "Epoch 79/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 17.6778 - r2_score: 0.7777 - val_loss: 17.5986 - val_r2_score: 0.7933\n",
            "Epoch 80/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 17.2087 - r2_score: 0.7990 - val_loss: 17.5491 - val_r2_score: 0.7939\n",
            "Epoch 81/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.0149 - r2_score: 0.7597 - val_loss: 17.4517 - val_r2_score: 0.7950\n",
            "Epoch 82/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 17.0277 - r2_score: 0.8029 - val_loss: 17.3792 - val_r2_score: 0.7959\n",
            "Epoch 83/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.1816 - r2_score: 0.7947 - val_loss: 17.3256 - val_r2_score: 0.7965\n",
            "Epoch 84/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 21.1837 - r2_score: 0.7734 - val_loss: 17.2440 - val_r2_score: 0.7975\n",
            "Epoch 85/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14.8586 - r2_score: 0.8288 - val_loss: 17.0535 - val_r2_score: 0.7997\n",
            "Epoch 86/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.8459 - r2_score: 0.7837 - val_loss: 16.9433 - val_r2_score: 0.8010\n",
            "Epoch 87/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20.5632 - r2_score: 0.7865 - val_loss: 16.7908 - val_r2_score: 0.8028\n",
            "Epoch 88/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21.1458 - r2_score: 0.7680 - val_loss: 16.6263 - val_r2_score: 0.8048\n",
            "Epoch 89/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16.5927 - r2_score: 0.7679 - val_loss: 16.4416 - val_r2_score: 0.8070\n",
            "Epoch 90/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16.5521 - r2_score: 0.7805 - val_loss: 16.2626 - val_r2_score: 0.8091\n",
            "Epoch 91/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 19.0975 - r2_score: 0.7710 - val_loss: 16.2254 - val_r2_score: 0.8095\n",
            "Epoch 92/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 19.1914 - r2_score: 0.7855 - val_loss: 16.2347 - val_r2_score: 0.8094\n",
            "Epoch 93/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.3509 - r2_score: 0.7952 - val_loss: 16.2359 - val_r2_score: 0.8094\n",
            "Epoch 94/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15.0138 - r2_score: 0.7987 - val_loss: 16.1332 - val_r2_score: 0.8106\n",
            "Epoch 95/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14.3655 - r2_score: 0.8214 - val_loss: 16.0184 - val_r2_score: 0.8120\n",
            "Epoch 96/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14.5881 - r2_score: 0.8150 - val_loss: 15.8795 - val_r2_score: 0.8136\n",
            "Epoch 97/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 18.5341 - r2_score: 0.7832 - val_loss: 15.6412 - val_r2_score: 0.8164\n",
            "Epoch 98/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16.6059 - r2_score: 0.8088 - val_loss: 15.6137 - val_r2_score: 0.8168\n",
            "Epoch 99/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.7166 - r2_score: 0.7933 - val_loss: 15.5155 - val_r2_score: 0.8179\n",
            "Epoch 100/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13.2143 - r2_score: 0.8364 - val_loss: 15.4316 - val_r2_score: 0.8189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGYPKelw_YrG",
        "outputId": "e47c19ce-fab5-464b-fcfe-d7d921322c90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The output values represent the loss (Mean Squarred Error) and\n",
        "# the metrics (Mean Absolute Error)\n",
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 20.0917 - r2_score: 0.7361\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[22.198041915893555, 0.7333369851112366]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMTQi9Cm_qTf",
        "outputId": "53ca7cdf-0561-439f-d76b-fcefb7cbfa88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we get a sample data (the first 2 inputs from the training data)\n",
        "to_predict = X_train_scaled[:2]\n",
        "\n",
        "predictions = model.predict(to_predict)\n",
        "print(predictions)\n",
        "print(y_train[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
            "[[13.716643]\n",
            " [37.412277]]\n",
            "[15.2 42.3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e9J3eOdUhJa"
      },
      "source": [
        "**Gradient Descent**\n",
        "\n",
        "* Most of the data science algorithms are optimization problems and one of the most used algorithms to do the same is the Gradient Descent Algorithm.\n",
        "\n",
        "* The **learning rate** is a tuning parameter in an optimization algorithm like Gradient descent that determines the step size at each iteration while moving toward a minimum\n",
        "\n",
        "* The learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum.\n",
        "\n",
        "* **Learning rate Momentum** both speeds up the learning (increasing the learning rate) when the error cost gradient is heading in the same direction for a long time and also avoids local minima by 'rolling over' small bumps.\n",
        "\n",
        "* **Learning rate Decay** serves to settle the learning in a nice place and avoid oscillations, a situation that may arise when a too high constant learning rate makes the learning jump back and forth over a minima\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTqXOyxj0yki"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVnh7hlj-Sh-"
      },
      "source": [
        "**Optimizer functions**\n",
        "\n",
        "* Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.\n",
        "\n",
        "* Gradient descent, Stochastic Gradient descent, Mini-Batch Gradient descent\n",
        "\n",
        "* Adaptive gradient descent algorithms such as Adagrad, RMSprop, Adam, provide an alternative to classical SGD. They have per-parameter learning rate methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb0tmnMZaXtv"
      },
      "source": [
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci4YIj_SZtpZ"
      },
      "source": [
        "* **Gradient Descent** calcultes gradient for the whole dataset and updates values in direction opposite to the gradients until we find a local minima.\n",
        "\n",
        "* In **Gradient Descent or Batch Gradient Descent**, we use the whole training data per epoch whereas, in **Stochastic Gradient Descent**, we use only single training example per epoch randomly (hence it fast) and **Mini-batch Gradient Descent** lies in between of these two extremes, in which we can use a mini-batch(small portion) of training data per epoch\n",
        "\n",
        "* Gradient descent is very basic and is seldom used now. One problem is with the global learning rate associated with the same.\n",
        "\n",
        "* **Adagrad** is more preferrable for a sparse data set as it makes big updates for infrequent parameters and small updates for frequent parameters. It uses a different learning rate for every parameter. Thus we do not need to manually tune the learning rate.\n",
        "\n",
        "* **RMSprop** and **Adadelta** have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates.\n",
        "\n",
        "* Adaptive Moment Estimation (**Adam**) is another method that computes adaptive learning rates for each parameter.\n",
        "\n",
        "* In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients.\n",
        "\n",
        "* Adam works well in practice, is faster, and outperforms other techniques.\n",
        "\n",
        "* Pick the optimizer that is newest because they usually report results on standard datasets, or best state of the art, or both.\n",
        "\n",
        "https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoM4AfXwRp-b"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLOjLtgCRrFu"
      },
      "source": [
        "**Hyperparameters**\n",
        "\n",
        "* Hyperparameters are parameters your neural network can’t learn itself via gradient descent or some other variant.\n",
        "\n",
        "* These include learning rate, number of layers, number of neurons in a given layer, dropout, Network Weight Initialization, Activation function, Momentum, Number of epochs, Batch size etc.\n",
        "\n",
        "* Tuning the hyperparameters refers to the process of choosing the best values of the hyperparameters.\n",
        "\n",
        "* Typically you evaluate the performance of the network on a validation set, and then tweak the hyperparameters and re-evaluate, choosing the values of the hyperparameters that give the best performance on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_GjvqbYeNb2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPeqB7AHfWwR"
      },
      "source": [
        "**How to Avoid Overfitting in Deep Learning Neural Networks**\n",
        "\n",
        "* The central challenge in machine learning is that we must perform well on new, previously unseen inputs — not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization.\n",
        "\n",
        "* We use methods like a train/test split or k-fold cross-validation to estimate the ability of the model to generalize to new data.\n",
        "\n",
        "* We can address **underfitting** by increasing the capacity of the model. Capacity refers to the ability of a model to fit a variety of functions; more capacity, means that a model can fit more types of functions for mapping inputs to outputs.\n",
        "\n",
        "* Increasing the capacity of a model is easily achieved by changing the structure of the model, such as adding more layers and/or more nodes to layers.\n",
        "\n",
        "* Because an underfit model is so easily addressed, it is more common to have an overfit model.\n",
        "\n",
        "* In case of **Overfitting**, line plots of the loss (that we seek to minimize) of the model on train and validation datasets will show a line for the training dataset that drops and may plateau and a line for the validation dataset that drops at first, then at some point begins to rise again.\n",
        "\n",
        "Overfitting can be addressed by various means like....\n",
        "\n",
        "* Constraining Model Complexity like no of  number of weights, magnitude of weights etc.\n",
        "\n",
        "* **Dropout**: Probabilistically remove inputs during training\n",
        "\n",
        "* **Early Stopping**: Monitor model performance on a validation set and stop training when performance degrades."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore Various model architectures at https://machinelearningmastery.com/keras-functional-api-deep-learning/"
      ],
      "metadata": {
        "id": "AUXm3Lq09YfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization**\n",
        "\n",
        "* Regularization is a technique which is used to solve the problem of Overfitting and High Variance.Overfitting is a phenomenon in machine learning,where model is very complex and tries to fit each training data.By covering each training data,it also picks up the noise points,which causes model to perform very good on training set,but performs very poor on testing set.Goal of Regularization is to solve this problem by making models simpler.\n",
        "\n",
        "* Regularization works by making models simpler.It achieves this by penalizing model parameters.If you know about Linear Regression,then you know that,while building the Regression models,each feature column is associated with some weights called model parameters.Depending upon this model parameters,slope of regression line is decided.If these model parameters are high,then slope of model will be more.If model parameter is very low,then slope of model will be less.\n",
        "\n",
        "* Regularization adds some amount of Bias to the model,to reduce the variance.By adding Bias to a high variance model,model does not fits to training data well and hence training accuracy will be low.But,it also reduces the Overfitting by reducing the Variance and this model will perform good in testing phase.\n",
        "\n",
        "* A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
        "\n",
        "* The key difference between these two is the penalty term.\n",
        "\n",
        "* Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
        "\n",
        "* Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n",
        "\n",
        "* The mean squared error (MSE) is the average squared error. The residual sum of squares (RSS) is the sum of the squared errors.\n",
        "\n",
        "* Learn more at https://www.einfochips.com/blog/regularization-make-your-machine-learning-algorithms-learn-not-memorize/\n"
      ],
      "metadata": {
        "id": "eWdy3qTB_Ebd"
      }
    }
  ]
}