{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBjMXwZN_UUH"
      },
      "source": [
        "###CNN s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RL8eUgNlkHqq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ8qONVDpBRA"
      },
      "source": [
        "* https://www.youtube.com/watch?v=f0t-OCG79-U\n",
        "\n",
        "* https://dev.to/sandeepbalachandran/machine-learning-going-furthur-with-cnn-part-2-41km\n",
        "\n",
        "* **Convolution** : Dictionary meaning - a thing that is complex and difficult to follow. a coil or twist\n",
        "\n",
        "* Convolutional Neural Networks are a powerful artificial neural network technique.\n",
        "\n",
        "* These networks preserve the spatial structure like edges etc. of the problem and were developed for object recognition tasks such as handwritten digit recognition.\n",
        "\n",
        "* Feature are learned and used across the whole image, allowing for the objects in the images to be shifted or translated in the scene and still detectable by the network.\n",
        "\n",
        "* It is this reason why the network is so useful for object recognition in photographs, picking out digits, faces, objects and so on with varying orientation.\n",
        "\n",
        "**There are three types of layers in a Convolutional Neural Network:**\n",
        "1. Convolutional Layers.\n",
        "2. Pooling Layers.\n",
        "3. Fully-Connected Layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "707o1tX4sxsh"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toC4LIPdsyos"
      },
      "source": [
        "**Convolutional layers are comprised of filters and feature maps.**\n",
        "\n",
        "\n",
        "* A convolution layer is like a flashlight that is shining over the top left of the image. Let’s say that the light this flashlight shines covers a 5 x 5 area. And now, let’s imagine this flashlight sliding across all the areas of the input image. In machine learning terms, this flashlight is called a filter(or sometimes referred to as a neuron or a kernel) and the region that it is shining over is called the receptive field.  The filters are essentially the neurons of the layer. They have both weighted inputs and generate an output value like a neuron.\n",
        "\n",
        "* The weights are adjusted in such a way as to learn / detect the patterns in the image like edges, shapes etc.\n",
        "\n",
        "* The feature map is the output of one filter applied to the previous layer. A given filter is drawn across the entire previous layer, moved one pixel at a time. Each position results in an activation of the neuron and the output is collected in the feature map.\n",
        "\n",
        "* The distance that filter is moved across the input from the previous layer at each activation is referred to as the stride.\n",
        "\n",
        "* Max pooling is a pooling operation that selects the maximum element from the region of the feature map covered by the filter. Thus, the output after max-pooling layer would be a feature map containing the most prominent features of the previous feature map.  It is to reduce overfitting and to increase computational speed.\n",
        "\n",
        "* Know more about other kinds of pooling at https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/\n",
        "\n",
        "* In between the convolutional layer and the fully connected layer, there is a 'Flatten' layer. Flattening transforms a two-dimensional matrix of features into a vector that can be fed into a fully connected neural network classifier.\n",
        "\n",
        "* Learn more at https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdRXJ05kYFR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2047543-1635-4711-881f-0f8347509c71"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the images.\n",
        "train_images = (train_images / 255)\n",
        "test_images = (test_images / 255)\n",
        "\n",
        "# Reshape the images. CNNs requires the third dimension.\n",
        "train_images = np.expand_dims(train_images, axis=3)\n",
        "test_images = np.expand_dims(test_images, axis=3)\n",
        "\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J07qKnYn2uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd852f3e-ccea-40f7-8bd2-1874a85eff2e"
      },
      "source": [
        "num_filters = 8\n",
        "filter_size = 3\n",
        "pool_size = 2\n",
        "\n",
        "# Build the model.\n",
        "model = Sequential([\n",
        "  Conv2D(\n",
        "    num_filters,\n",
        "    filter_size,\n",
        "    input_shape=(28, 28, 1),\n",
        "    strides=1,\n",
        "    ),\n",
        "  MaxPooling2D(pool_size=pool_size),\n",
        "  Flatten(),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "  'adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  epochs=5,\n",
        "  validation_data=(test_images, to_categorical(test_labels)),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.8761 - loss: 0.4564 - val_accuracy: 0.9656 - val_loss: 0.1180\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.9696 - loss: 0.1034 - val_accuracy: 0.9768 - val_loss: 0.0763\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9793 - loss: 0.0679 - val_accuracy: 0.9779 - val_loss: 0.0730\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 12ms/step - accuracy: 0.9855 - loss: 0.0463 - val_accuracy: 0.9814 - val_loss: 0.0659\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9887 - loss: 0.0339 - val_accuracy: 0.9825 - val_loss: 0.0605\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7891d4d52ea0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzrns8rqg8Th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae3f8f6-157b-49f9-df44-8eab6019b855"
      },
      "source": [
        "# Predict on the first 5 test images.\n",
        "predictions = model.predict(test_images[:5])\n",
        "\n",
        "# Print our model's predictions.\n",
        "print(predictions)\n",
        "print(np.argmax(predictions, axis=1))\n",
        "\n",
        "# Check our predictions against the ground truths.\n",
        "print(test_labels[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "[[2.73254250e-06 2.05739775e-06 8.47558840e-05 1.98799080e-05\n",
            "  4.04923028e-08 4.47422259e-08 2.98449737e-11 9.99889255e-01\n",
            "  2.92799569e-07 9.53716949e-07]\n",
            " [3.28461942e-06 6.02812588e-06 9.99990463e-01 1.47014774e-07\n",
            "  7.51293576e-15 2.22567031e-08 1.44515315e-08 4.87178454e-14\n",
            "  1.07431581e-11 1.36278087e-13]\n",
            " [1.33366063e-06 9.99545276e-01 7.56561349e-05 3.63098678e-07\n",
            "  1.03233295e-04 6.59871603e-06 2.23877669e-05 2.15372012e-04\n",
            "  2.94564397e-05 3.28214725e-07]\n",
            " [9.99960542e-01 1.72589250e-06 3.21028565e-05 1.80830762e-06\n",
            "  4.58225990e-08 5.76891637e-07 1.48849790e-06 9.79786478e-07\n",
            "  1.53556776e-08 6.91761784e-07]\n",
            " [1.68974779e-07 1.92328457e-06 1.41365874e-07 1.11973669e-07\n",
            "  9.99594748e-01 6.53683810e-06 5.53490338e-07 1.46658540e-05\n",
            "  7.32418300e-07 3.80494195e-04]]\n",
            "[7 2 1 0 4]\n",
            "[7 2 1 0 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More examples**\n",
        "\n",
        "https://www.tensorflow.org/tutorials/images/cnn"
      ],
      "metadata": {
        "id": "_sXfhqIL1uV4"
      }
    }
  ]
}